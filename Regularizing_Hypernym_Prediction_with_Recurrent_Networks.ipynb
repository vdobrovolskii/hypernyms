{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regularizing Hypernym Prediction with Recurrent Networks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vdobrovolskii/hypernyms/blob/master/Regularizing_Hypernym_Prediction_with_Recurrent_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ohGb9l968Z13",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Regularizing Hypernym Prediction with Recurrent Networks"
      ]
    },
    {
      "metadata": {
        "id": "rosJErQf8Wtr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the environment"
      ]
    },
    {
      "metadata": {
        "id": "zQA8ZaNS7nGl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim keras pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wjK99lQ37D95",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pickle\n",
        "import random\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from xml.etree import ElementTree as ET\n",
        "from math import ceil\n",
        "\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle as shuffle_multiple_arrays\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Dense, Input, PReLU\n",
        "from keras.losses import cosine_proximity\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wXNH-E9v-zzr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', 500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bO5kAYQx7ZNz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the following cell if you are planning to use Google Colab"
      ]
    },
    {
      "metadata": {
        "id": "4mraaAXV7Dss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "# Modify the variable for it to contain the directory where the files are located\n",
        "path = \"/content/gdrive/My Drive/Colab Notebooks/rhpwrn/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MHMexD1o8O-j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helpers"
      ]
    },
    {
      "metadata": {
        "id": "JO21gD3_g3g5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_vocab(vectors):\n",
        "    \"\"\"\n",
        "    Creating a dictionary of all the entries in the object holding the vectors.\n",
        "    For instance, for vectors of \"мама_NOUN#7\", \"мама_NOUN#9\", \"мама_NOUN#10\" we get:\n",
        "    \"мама_NOUN\" : [\"7\", \"9\", \"10\"]\n",
        "    \n",
        "    Parameters:\n",
        "        vectors (gensim.models.KeyedVectors): sense embeddings object. Refer to\n",
        "            https://radimrehurek.com/gensim/models/keyedvectors.html\n",
        "        \n",
        "    Returns:\n",
        "        dict: see above\n",
        "    \"\"\"\n",
        "    word_sense_pattern = re.compile('(.+)#(\\d+)')\n",
        "    vocab = defaultdict(lambda: [])\n",
        "    for key in vectors.vocab.keys():\n",
        "        match_obj = re.match(word_sense_pattern, key)\n",
        "        vocab[match_obj.group(1)].append(match_obj.group(2))\n",
        "    return dict(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8kB0Vyt79PmS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def iter_wiktionary(filename, prefix):\n",
        "    \"\"\"\n",
        "    Memory-friendly xml iteratation.\n",
        "    \n",
        "    Parameters:\n",
        "        filename (str): the name of the file containing the xml dump of Wiktionary.\n",
        "        prefix (str): the prefix used in each tag of the xml file.\n",
        "        \n",
        "    Yields:\n",
        "        xml.etree.ElementTree.Element: element tagged \"page\"\n",
        "    \"\"\"\n",
        "    context = iter(ET.iterparse(filename, events=(\"start\", \"end\")))\n",
        "    event, root = context.__next__()\n",
        "    for event, elem in context:\n",
        "        if event == \"end\" and elem.tag == prefix + \"page\":\n",
        "            yield elem\n",
        "            elem.clear()\n",
        "            root.clear()\n",
        "\n",
        "            \n",
        "def parse_wiktionary(filename, vocab, verbose=False):\n",
        "    \"\"\"\n",
        "    Creates a dictionary with all the hyponym-hypernym pairs found in Wiktionary\n",
        "    for which there exist trained vectors.\n",
        "    \n",
        "    Parameters:\n",
        "        filename (str): the name of the file containing the xml dump of Wiktionary.\n",
        "        vocab (dict or set): an object containing all the words that we have vectors for.\n",
        "        verbose (bool): whether or not to display progress.\n",
        "        \n",
        "    Returns:\n",
        "        dict: a dictionary where every key is a vocab item and every value is a list of sets\n",
        "            for example: \"ключ_NOUN\": [set([\"инструмент_NOUN, приспособление_NOUN\"]),\n",
        "                                       set([\"водоем_NOUN\"]),\n",
        "                                       set([\"знак_NOUN\", \"символ_NOUN\"])]\n",
        "    \n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\n===== Parsing Wiktionary =====\")\n",
        "        \n",
        "    POS = {\"сущ\":\"NOUN\",\n",
        "           \"собств\":\"PROPN\",\n",
        "           \"гл\":\"VERB\",\n",
        "           \"прил\":\"ADJ\",\n",
        "           \"числ\":\"NUM\",\n",
        "           \"мест\":\"ADJ\",\n",
        "           \"прич\":\"ADJ\"}\n",
        "    prefix = \"{http://www.mediawiki.org/xml/export-0.10/}\"\n",
        "    section_pattern = re.compile(r\"^=== Морфологические и синтаксические свойства ===\\n(?:[^\\n]*?\\n)*?{{([а-я]+)\"\n",
        "                                 + r\" ru.+?\"\n",
        "                                 + r\"^==== Гиперонимы ====\\n((?:#[^\\n]*\\n)+)\"\n",
        "                                 + r\".+?\"\n",
        "                                 + r\"^==== Гипонимы ====\\n((?:#[^\\n]*\\n)+)\", flags=re.M|re.S)\n",
        "    hypernym_pattern = re.compile(r\"\\[\\[([A-Za-zА-ЯЁа-яё-]+)\\]\\]\")\n",
        "    yo_pattern = re.compile(r\"ё\", flags=re.I)\n",
        "     \n",
        "    to_hypernyms = defaultdict(lambda: [])\n",
        "    to_hyponyms = defaultdict(lambda: []) ###\n",
        "    counter = 0\n",
        "    \n",
        "    for page in iter_wiktionary(filename, prefix):\n",
        "        title = page.find(prefix + \"title\").text\n",
        "        text = page.find(prefix + \"revision\").find(prefix + \"text\").text\n",
        "        if text is None:\n",
        "            continue\n",
        "        matches = re.findall(section_pattern, text)\n",
        "        if not matches:\n",
        "            continue\n",
        "        for match in matches:\n",
        "            pos = match[0]\n",
        "            if not pos in POS:\n",
        "                continue\n",
        "            if pos == 'сущ' and title[0].isupper():\n",
        "                pos = 'собств'\n",
        "            processed_title = re.sub(yo_pattern, 'е', title.lower()) + f\"_{POS[pos]}\"\n",
        "            if not processed_title in vocab:\n",
        "                    continue\n",
        "            for hypernyms_group in match[1].splitlines():\n",
        "                hypernyms = re.findall(hypernym_pattern, hypernyms_group)\n",
        "                if not hypernyms:\n",
        "                    continue                \n",
        "                filtered_hypernyms = set()\n",
        "                for hypernym in hypernyms:\n",
        "                    hypernym = re.sub(yo_pattern, 'е', hypernym.lower()) + f\"_{POS[pos if pos != 'собств' else 'сущ']}\"\n",
        "                    if hypernym in vocab:\n",
        "                        filtered_hypernyms.add(hypernym)\n",
        "                if not filtered_hypernyms:\n",
        "                    continue\n",
        "                to_hypernyms[processed_title].append(filtered_hypernyms)\n",
        "            for hyponyms_group in match[2].splitlines():\n",
        "                hyponyms = re.findall(hypernym_pattern, hyponyms_group)\n",
        "                if not hyponyms:\n",
        "                    continue                \n",
        "                filtered_hyponyms = set()\n",
        "                for hyponym in hyponyms:\n",
        "                    hyponym = re.sub(yo_pattern, 'е', hyponym.lower()) + f\"_{POS[pos if pos != 'собств' else 'сущ']}\"\n",
        "                    if hyponym in vocab:\n",
        "                        filtered_hyponyms.add(hyponym)\n",
        "                if not filtered_hyponyms:\n",
        "                    continue\n",
        "                to_hyponyms[processed_title].append(filtered_hyponyms)\n",
        "                if verbose:\n",
        "                    counter += 1\n",
        "                    if counter % 100 == 0:\n",
        "                        print(f\"{counter}\\n\" if counter % 10000 == 0 else '.', end='', flush=True)\n",
        "    if verbose:\n",
        "        print(counter)\n",
        "    return dict(to_hypernyms), dict(to_hyponyms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dr55g-7L8V_B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BasicQueue:\n",
        "    \"\"\"\n",
        "    Non-complicated queue to perform BFS with.\n",
        "    \n",
        "    Methods:\n",
        "        __init__(iterable)\n",
        "        push(obj): add one object to queue\n",
        "        extend(iterable): add multiple objects to queue\n",
        "        __next__()\n",
        "        __iter__()\n",
        "    \"\"\"\n",
        "    def __init__(self, iterable):\n",
        "        self._list = []\n",
        "        self._i = 0\n",
        "        self.extend(iterable)\n",
        "        \n",
        "    def push(self, obj):\n",
        "        self._list.append(obj)\n",
        "        \n",
        "    def extend(self, iterable):\n",
        "        self._list.extend(iterable)\n",
        "        \n",
        "    def __next__(self):\n",
        "        if self._i >= len(self._list):\n",
        "            raise StopIteration\n",
        "        obj = self._list[self._i]\n",
        "        self._i += 1\n",
        "        if self._i == 2048:\n",
        "            self._list = self._list[2048:]\n",
        "            self._i = 0\n",
        "        return obj\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    \n",
        "class VectorNet:\n",
        "    \"\"\"\n",
        "    WordNet-like hierarchy where the nodes are sense vectors.\n",
        "    \n",
        "    Methods:\n",
        "        __init__(to_hypernyms, to_hyponyms, vectors, vocab)\n",
        "        build_training_data(test_size=0.025)\n",
        "    \"\"\"\n",
        "    def __init__(self, to_hypers, to_hypos, vectors, vocab):\n",
        "        self._vectors = vectors\n",
        "        self._vocab = vocab\n",
        "        self._up = defaultdict(lambda: set())\n",
        "        self._down = defaultdict(lambda: set())\n",
        "        for hyper, hyposets in to_hypos.items():\n",
        "            for hyposet in hyposets:\n",
        "                for hypo in hyposet:\n",
        "                    self._up[hypo].add(hyper)\n",
        "                    self._down[hyper].add(hypo)\n",
        "        for hypo, hypersets in to_hypers.items():\n",
        "            for hyperset in hypersets:\n",
        "                for hyper in hyperset:\n",
        "                    self._up[hypo].add(hyper)\n",
        "                    self._down[hyper].add(hypo)\n",
        "        self._up = dict(self._up)\n",
        "        self._down = dict(self._down)\n",
        "        self._remove_extraneous_links()\n",
        "        self._vup = dict()\n",
        "        self._vdown = defaultdict(lambda: set())\n",
        "        self._vectorize_data()\n",
        "        \n",
        "    def _is_hyper(self, w1, w2):\n",
        "        \"\"\"\n",
        "        Checks if w1 is a hypernym of w2.\n",
        "        \"\"\"\n",
        "        i = 0\n",
        "        processed = set()\n",
        "        parents = BasicQueue([w2])\n",
        "        for parent in parents:\n",
        "            processed.add(parent)\n",
        "            if not parent in self._up:\n",
        "                continue\n",
        "            if w1 in self._up[parent]:\n",
        "                return True\n",
        "            for new in self._up[parent]:\n",
        "                if not new in processed:\n",
        "                    parents.push(new)\n",
        "        return False\n",
        "        \n",
        "    def _remove_extraneous_links(self):\n",
        "        \"\"\"\n",
        "        Removes recursion in the network.\n",
        "        \"\"\"\n",
        "        for word, hypers in self._up.items():          \n",
        "            if word in hypers:\n",
        "                hypers.remove(word)\n",
        "                self._down[word].remove(word)\n",
        "            if word in self._down:\n",
        "                bugs = hypers.intersection(self._down[word])\n",
        "                if bugs:\n",
        "                    hypers.difference_update(bugs)\n",
        "            for hyper in hypers:\n",
        "                if self._is_hyper(word, hyper):\n",
        "                    for hypo in self._down[word].copy():\n",
        "                        if self._is_hyper(hypo, hyper):\n",
        "                            self._down[word].remove(hypo)\n",
        "                            self._up[hypo].remove(word)                           \n",
        "            to_remove = set()\n",
        "            for hyper1 in hypers:\n",
        "                for hyper2 in hypers - set([hyper1]):\n",
        "                    if self._is_hyper(hyper2, hyper1):\n",
        "                        to_remove.add(hyper2)\n",
        "            for hyper in to_remove:\n",
        "                if hyper in self._down:\n",
        "                        self._down[hyper].remove(word)\n",
        "                hypers.remove(hyper)\n",
        "                \n",
        "    def _find_optimal_pair(self, first, second):\n",
        "        best = (0, None, None)\n",
        "        for elem1 in first:\n",
        "            for elem2 in second:\n",
        "                similarity = self._vectors.similarity(elem1, elem2)\n",
        "                if similarity > best[0]:\n",
        "                    best = (similarity, elem1, elem2)\n",
        "        return best\n",
        "                \n",
        "    def _get_hypernym(self, sense, level=1):\n",
        "        current = sense\n",
        "        for _ in range(level):\n",
        "            if current in self._vup:\n",
        "                current = self._vup[current]\n",
        "            else:\n",
        "                current = None\n",
        "                break\n",
        "        return current\n",
        "    \n",
        "    def _get_senses(self, item):\n",
        "        if isinstance(item, str):\n",
        "            return [f\"{item}#{sense}\" for sense in self._vocab[item]]\n",
        "        elif isinstance(item, set):\n",
        "            return [f\"{word}#{sense}\" for word in item for sense in self._vocab[word]]\n",
        "        else:\n",
        "            raise ValueError(\"item must be of type set or str.\")\n",
        "                \n",
        "    def _vectorize_data(self):\n",
        "        for word, hypers in self._up.items():\n",
        "            hypersenses = self._get_senses(hypers)\n",
        "            for sense in self._get_senses(word):\n",
        "                score, sense, hypersense = self._find_optimal_pair([sense], hypersenses)\n",
        "                if score > 0.6:                  \n",
        "                    self._vup[sense] = hypersense\n",
        "                    self._vdown[hypersense].add(sense)\n",
        "                    \n",
        "    def build_training_data(self, max_depth=9, test_size=0.025):\n",
        "        \"\"\"\n",
        "        Builds and returns the data to be used for training and evaluating a NN.\n",
        "        \n",
        "        Parameters:\n",
        "            max_depth (int): maximum hypernymy depth (min 1).\n",
        "            test_size (float): range (0;1), the amount of data used for tests.\n",
        "            \n",
        "        Returns:\n",
        "            tuple:\n",
        "                x_train (numpy.ndarray): vectors fed to the models, shape (n_samples, n_features).\n",
        "                y_train (numpy.ndarray): expected vectors, shape (n_samples, n_features).\n",
        "                levels_train (numpy.ndarray): hypernymy depth for each sample pair, shape (n_samples, 1).\n",
        "                x_test_vec (numpy.ndarray): data used for tests, shape (n_samples, n_features).\n",
        "                x_test_str (list): list of str containing vector labels of the test data.\n",
        "                y_test_str (list): list of str containing expected vector labels.\n",
        "        \"\"\"\n",
        "        if max_depth < 1:\n",
        "            raise ValueError(\"max_depth must be 1 or greater\")\n",
        "        if test_size <= 0 or test_size >= 1:\n",
        "            raise ValueError(\"test_size must be in range (0;1)\")\n",
        "        x_train = []\n",
        "        y_train = []\n",
        "        levels_train = []\n",
        "        x_test_vec = []\n",
        "        x_test_str = []\n",
        "        y_test_str = []\n",
        "        senses = list(self._vup.keys())\n",
        "        random.shuffle(senses)\n",
        "        for i, sense in enumerate(senses):\n",
        "            if i < len(senses) * test_size:\n",
        "                x_test_str.append(sense)\n",
        "                x_test_vec.append(self._vectors[sense])\n",
        "                y_test_str.append(set())\n",
        "            current_level = 0\n",
        "            while current_level < max_depth:\n",
        "                current_level += 1\n",
        "                hypernym = self._get_hypernym(sense, current_level)\n",
        "                if hypernym:\n",
        "                    if i < len(senses) * test_size:\n",
        "                        y_test_str[i].add(hypernym)\n",
        "                    else:\n",
        "                        x_train.append(self._vectors[sense])\n",
        "                        y_train.append(self._vectors[hypernym])\n",
        "                        levels_train.append(current_level)\n",
        "                else:\n",
        "                    break\n",
        "        x_train = np.stack(x_train, axis=0)\n",
        "        y_train = np.stack(y_train, axis=0)\n",
        "        levels_train = np.stack(levels_train, axis=0)\n",
        "        x_test_vec = np.stack(x_test_vec, axis=0)\n",
        "        return x_train, y_train, levels_train, x_test_vec, x_test_str, y_test_str"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LYqZBcNG9m4T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LeveledModel:\n",
        "    def __init__(self, x, y, levels, vectors):\n",
        "        x, y, levels = shuffle_multiple_arrays(x, y, levels, random_state=12345)\n",
        "        self._models = dict()\n",
        "        self._data_train = dict()\n",
        "        self._depth_fractions_train = dict()\n",
        "        self._vectors = vectors\n",
        "        print(f\"The deepest level is {np.max(np.unique(levels))}\")\n",
        "        recurrent_unit = [Dense(1200), PReLU(), Dense(600), PReLU(), Dense(600), PReLU(), Dense(300)]\n",
        "        for depth in np.unique(levels):            \n",
        "            self._models[depth] = self._build_model(recurrent_unit, depth)\n",
        "            mask_train = levels == depth            \n",
        "            self._data_train[depth] = (x[mask_train], y[mask_train])\n",
        "            self._depth_fractions_train[depth] = x[mask_train].shape[0] / x.shape[0]\n",
        "            print(f\"    Train {depth:>2}: {x[mask_train].shape[0]:>5} examples\")\n",
        "            \n",
        "    def predict(self, x, confidence_threshold=0.95, verbose=False):\n",
        "        total_models = len(list(self._models.keys())) # calculate it elsewhere and keep?\n",
        "        raw_predictions = [None for _ in range(total_models)]\n",
        "        predictions = []\n",
        "        confidence_data = []\n",
        "        all_predictions = []\n",
        "        if verbose:\n",
        "            print(\"Getting raw predictions \", end='')\n",
        "        for i, model in self._models.items():\n",
        "            if verbose:\n",
        "                print('.', end='')\n",
        "            raw_predictions[i - 1] = model.predict(x)\n",
        "        print(\" Done\\n\\nCalculating distances and choosing the most appropriate prediction...\")\n",
        "        for i in range(x.shape[0]):\n",
        "            for model_id in range(total_models):\n",
        "                hyper, confidence = self._vectors.most_similar(positive=[raw_predictions[model_id][i]], topn=1)[0]           \n",
        "                if model_id == 0:\n",
        "                    predictions.append(hyper)\n",
        "                    confidence_data.append(confidence)\n",
        "                    all_predictions.append([])\n",
        "                elif confidence_data[i] < confidence_threshold and confidence > confidence_data[i]:\n",
        "                    predictions[i] = hyper\n",
        "                    confidence_data[i] = confidence\n",
        "                all_predictions[i].append((model_id + 1, hyper, confidence))\n",
        "            if verbose:\n",
        "                print(f\"{i + 1}\\n\" if (i + 1) % 100 == 0 else '.', end='')\n",
        "        if verbose:\n",
        "            print((i + 1) if (i + 1) % 100 != 0 else '')\n",
        "        return predictions, confidence_data, all_predictions  \n",
        "            \n",
        "    def evaluate(self, all=False):\n",
        "        if not all:\n",
        "            print(\"Warning: evaluating only 1st level hypernyms.\")\n",
        "            return self._models[1].evaluate(*self._data_test[1])\n",
        "            \n",
        "    def fit(self, epochs=1, validation_split=0.05):\n",
        "        for i in range(epochs):\n",
        "            acc = 0\n",
        "            loss = 0\n",
        "            val_acc = 0\n",
        "            val_loss = 0\n",
        "            \n",
        "            print(f\"Epoch {i + 1:>4}: \", end='')\n",
        "            for depth, model in self._models.items():    \n",
        "                history = model.fit(*self._data_train[depth], validation_split=validation_split, epochs=1, verbose=0, batch_size=1024)\n",
        "                acc += history.history['acc'][-1] * self._depth_fractions_train[depth]\n",
        "                loss += history.history['loss'][-1] * self._depth_fractions_train[depth]\n",
        "                val_acc += history.history['val_acc'][-1] * self._depth_fractions_train[depth]\n",
        "                val_loss += history.history['val_loss'][-1] * self._depth_fractions_train[depth]\n",
        "                print('.', end='')\n",
        "            print(f\" Loss: {loss:7.5f}, Accuracy: {acc:7.5f}, Val. loss: {val_loss:7.5f}, Val. accuracy: {val_acc:7.5f}\")\n",
        "            \n",
        "            \n",
        "    @staticmethod\n",
        "    def _build_model(recurrent_unit, depth):\n",
        "        inputs = Input(shape=(300,))\n",
        "        x = inputs\n",
        "        for _ in range(depth):\n",
        "            for layer in recurrent_unit:\n",
        "                x = layer(x)\n",
        "        outputs = x\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        model.compile(optimizer=\"rmsprop\",\n",
        "                     loss=lambda y_true, y_pred: weighted_cosine(y_true, y_pred, 2 ** depth),\n",
        "                     metrics=[\"accuracy\"])\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sFDPs2Ka9xs0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weighted_cosine(y_true, y_pred, weight):\n",
        "    \"\"\"\n",
        "    Custom weight function to be used in keras model training.\n",
        "    Weights the standard cosine proximity by a parameter.\n",
        "    More specifically, takes the cosine distance (1 - cosine_proximity), weighs\n",
        "    it and transforms back to cosine_proximity. Also note that the standard keras\n",
        "    cosine proximity is multiplied by -1 to target for minimizing the function.\n",
        "    \"\"\"\n",
        "    cosine_distance = 1 + cosine_proximity(y_true, y_pred)\n",
        "    return -(1 - cosine_distance * weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f629dpBG-oQB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_model(x_str, y_true_str, y_pred_str, confidence_data, all_preds, vectors):\n",
        "    \"\"\"\n",
        "    Outputs information on the model's performance.\n",
        "    \n",
        "    Parameters:\n",
        "        x_str (list): labels of vectors fed to the model.\n",
        "        y_true_str (list): labels of vectors that are expected from the model.\n",
        "        y_pred_str (list): labels of vectors that the model has predicted.\n",
        "        confidence_data (list): cosines between raw predictions and predicted labels.\n",
        "        \n",
        "            Note: lengths of all the previous parameters should be equal.\n",
        "            \n",
        "        vectors (gensim.models.KeyedVectors): sense embeddings object. Refer to\n",
        "            https://radimrehurek.com/gensim/models/keyedvectors.html\n",
        "        \n",
        "    Returns:\n",
        "        pandas.DataFrame: \"auto_eval\": are_same(standard, predicted)\n",
        "                          \"stimuli\": label of sample x\n",
        "                          \"standard\": label of sample y\n",
        "                          \"predicted\": predicted label\n",
        "                          \"similarity\": cosine(standard, predicted)\n",
        "                          \"confidence\": cosine(raw_prediction, predicted)\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for i in range(len(x_str)):\n",
        "        stimulus = x_str[i]\n",
        "        standard = y_true_str[i]\n",
        "        predicted = y_pred_str[i]\n",
        "        confidence = confidence_data[i]\n",
        "        auto_eval = predicted in standard\n",
        "        rows.append(pd.Series({\"auto_eval\":auto_eval,\n",
        "                   \"stimuli\":stimulus,\n",
        "                   \"standard\":\", \".join(standard),\n",
        "                   \"predicted\":predicted,\n",
        "                   \"all_preds\":\", \".join([f\"({e[0]}, {e[1]}, {e[2]:1.5f})\" for e in all_preds[i]]),\n",
        "                   \"confidence\":confidence}))\n",
        "        print('.' if auto_eval else 'x', end='\\n' if (i + 1) % 100 == 0 else '')\n",
        "    df = pd.DataFrame(rows)\n",
        "    print(\"\\n\")\n",
        "    print(f\"Correct:         {df['auto_eval'].sum()} out of {len(df)}\")\n",
        "    print()\n",
        "    print(\"Be careful when interpreting the results as they still require human evaluation.\")\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0zW4HaYz8f3g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "n1a03BQP7Dp0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VECTOR_FILE = path + \"sense_vectors.bin\"\n",
        "WIKI_FILE = path + \"wiktionary.xml\"\n",
        "PARSED_WIKI = path + \"parsed-wiki.pickle\"\n",
        "TRAINING_DATA = path + \"data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AM1iOgq17Dmr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectors = KeyedVectors.load_word2vec_format(VECTOR_FILE, binary=True, encoding='utf8', unicode_errors='ignore') \n",
        "vectors.init_sims(replace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jzIpj7WK9TCK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab = build_vocab(vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7RpcbNW8Q-sK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uncomment one of the following to either parse the Wiktionary database again or to use the pre-parsed data."
      ]
    },
    {
      "metadata": {
        "id": "eBAKQV0_9Vr5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#to_hypers, to_hypos = parse_wiktionary(WIKI_FILE, vocab, verbose=True)\n",
        "\n",
        "#with open(PARSED_WIKI, mode=\"rb\") as f:\n",
        "#    to_hypers, to_hypos = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zlmTGi5D92kS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vn = VectorNet(to_hypers, to_hypos, vectors, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EXGVJCgPRQcx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uncomment one of the following to either rebuild the training data or use the pre-built one."
      ]
    },
    {
      "metadata": {
        "id": "29B3e_bp95UD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#x_train, y_train, levels_train, x_test_vec, x_test_str, y_test_str = vn.build_training_data()\n",
        "\n",
        "#with open(path + \"data\", mode=\"rb\") as f:\n",
        "#    x_train, y_train, levels_train, x_test_vec, x_test_str, y_test_str = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eu4eJlEm-W_M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LeveledModel(x_train, y_train, levels_train, vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hCKJ01wsRZzl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uncomment one of the following to either retrain the model or use the pre-trained one."
      ]
    },
    {
      "metadata": {
        "id": "TaJTKxIC-eX_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model.fit(epochs=500)\n",
        "\n",
        "#for i in range(1, 10):\n",
        "#    model._models[i].load_weights(path + \"model_weights\" + str(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J1SysgnW8jMh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "xiJCSCugXwpo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred_str, confidence_data, all_preds = model.predict(x_test_vec, confidence_threshold=0.97, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lucFAO8CnBG1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evaluate_model(x_test_str, y_test_str, y_pred_str, confidence_data, all_preds, vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
